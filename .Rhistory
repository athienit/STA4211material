fun = mean,
geom = "point",
shape = 17,  # Use a triangle as the point shape
size = 3,
color = "red",  # Set the color to red
alpha=0.07, #opacity
position = position_dodge(width = 0.75)  # Adjust the dodge width as needed
) +
coord_cartesian(xlim = c(1.2, NA), clip = "off")
ggplot(yields, aes(x = Condition, y = Yields,fill=Condition)) +
ggdist::stat_halfeye(
adjust = .9, #custom bandwidth
width = .6,
.width = 0,
justification = -.3,
point_colour = NA) +
geom_boxplot(
width = .25,
outlier.shape = NA
) +
geom_point(
size = 1.3,
alpha = .3,
position = position_jitter(
seed = 1, width = .1
)
) +
stat_summary(
fun = mean,
geom = "point",
shape = 17,  # Use a triangle as the point shape
size = 3,
color = "red",  # Set the color to red
alpha=0.7, #opacity
position = position_dodge(width = 0.75)  # Adjust the dodge width as needed
) +
coord_cartesian(xlim = c(1.2, NA), clip = "off")
### Some cross-validation
# Load necessary libraries
library(dplyr) # For data manipulation
ddply(yields,.(Condition),summarise,means=mean(Yields),sds=sd(Yields))
library(caret) # For cross-validation
install.packages("caret")
help(createFolds)
# Define the number of folds
train_control <- trainControl(method = "cv",
number = 10)
library(MASS)  # For the Boston dataset
library(dplyr) # For data manipulation
library(caret) # For cross-validation
# Load your dataset (replace 'Boston' with your dataset)
data(Boston)
df <- Boston
# Set the seed for reproducibility
set.seed(123)
# Define the number of folds
num_folds <- 10
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(df$medv, k = num_folds, list = TRUE, returnTrain = FALSE)
folds
### Some cross-validation
# Load necessary libraries
library(dplyr) # For data manipulation
library(caret) # For cross-validation
# Define the number of folds
num_folds <- 10
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(df$medv, k = num_folds, list = TRUE, returnTrain = FALSE)
folds
dim(folds)
dim(folds$Fold10)
length(folds$Fold10)
df
View(df)
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(yields$Yields, k = num_folds, list = TRUE, returnTrain = FALSE)
folds
help(createFolds)
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(yields$Yields, k =10, p=0.7, list = TRUE, returnTrain = FALSE)
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times =10, p=0.7, list = TRUE)
folds
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times =1, p=0.7, list = TRUE)
folds
num_folds=10
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times =num_folds, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
valid_data <- df[partitions == i, ]
# Fit the model on the training data using aov
model <- aov(medv ~., data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$medv - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Create indices for stratified 10-fold cross-validation
partitions <- createDataPartition(yields$Yields, times =num_folds, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
valid_data <- df[partitions == i, ]
# Fit the model on the training data using aov
model <- aov(medv ~., data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$medv - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Create indices for stratified 10-fold cross-validation
partitions <- createDataPartition(yields$Yields, times=1, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
valid_data <- df[partitions == i, ]
# Fit the model on the training data using aov
model <- aov(medv ~., data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$medv - predictions)^2)
# Store the result
cv_results[i] <- mse
}
num_folds=10
# Create indices for stratified 10-fold cross-validation
partitions <- createDataPartition(yields$Yields, times=1, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
valid_data <- df[partitions == i, ]
# Fit the model on the training data using aov
model <- aov(medv ~., data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$medv - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Calculate the mean and standard deviation of the cross-validation results
mean_mse <- mean(cv_results)
# Load necessary libraries
library(MASS)  # For the Boston dataset
library(dplyr) # For data manipulation
library(caret) # For cross-validation
# Load your dataset (replace 'Boston' with your dataset)
data(Boston)
df <- Boston
# Set the seed for reproducibility
set.seed(123)
# Define the number of folds
num_folds <- 10
# Create data partitions for stratified 10-fold cross-validation
partitions <- createDataPartition(df$medv, p = 0.9, list = FALSE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
valid_data <- df[partitions == i, ]
# Fit the model on the training data using aov
model <- aov(medv ~., data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$medv - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Calculate the mean and standard deviation of the cross-validation results
mean_mse <- mean(cv_results)
std_mse <- sd(cv_results)
# Print the results
cat("Mean MSE:", mean_mse, "\n")
cat("Standard Deviation of MSE:", std_mse, "\n")
num_folds=10
# Create indices for stratified 10-fold cross-validation
partitions <- createDataPartition(yields$Yields, times=num_folds, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
i=1
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
num_folds=1
# Create indices for stratified 10-fold cross-validation
partitions <- createDataPartition(yields$Yields, times=num_folds, p=0.7, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Split the data into training and validation sets
train_data <- df[partitions != i, ]
partitions
### Some cross-validation
# Load necessary libraries
library(dplyr) # For data manipulation
library(caret) # For cross-validation
# Define the number of folds
num_folds <- 10
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(df$medv, k = num_folds, list = TRUE, returnTrain = FALSE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- df[-folds[[i]], ]
valid_data <- df[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Calculate the mean and standard deviation of the cross-validation results
mean_mse <- mean(cv_results)
std_mse <- sd(cv_results)
# Print the results
cat("Mean MSE:", mean_mse, "\n")
cat("Standard Deviation of MSE:", std_mse, "\n")
cv_results
i=1
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
train_data
folds[[i]]
folds
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(yields$Yields, k = num_folds, list = TRUE, returnTrain = FALSE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
}
# Calculate the mean and standard deviation of the cross-validation results
mean_mse <- mean(cv_results)
std_mse <- sd(cv_results)
# Print the results
cat("Mean MSE:", mean_mse, "\n")
cat("Standard Deviation of MSE:", std_mse, "\n")
cv_results
# Create indices for stratified 10-fold cross-validation
folds <- createFolds(yields$Yields, k = num_folds, list = TRUE, returnTrain = FALSE)
folds
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, k = num_folds, p=0.8, list = TRUE, returnTrain = FALSE)
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times = num_folds, p=0.8, list = TRUE, returnTrain = FALSE)
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times = num_folds, p=0.8, list = TRUE)
folds
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
}
i
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
train_data
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times = num_folds, p=0.8, list = TRUE)
folds
i=1
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
summary(model)
0.8*50
# Create indices for stratified 10-fold cross-validation
folds <- createDataPartition(yields$Yields, times = num_folds, p=0.2, list = TRUE)
# Initialize a vector to store cross-validation results
cv_results <- numeric(num_folds)
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
train_data
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
cv_results
i=2
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Split the data into training and validation sets
train_data <- yields[-folds[[i]], ]
valid_data <- yields[folds[[i]], ]
# Fit the model on the training data using aov
model <- aov(Yields ~Condition, data = train_data)
# Make predictions on the validation set
predictions <- predict(model, newdata = valid_data)
# Calculate the mean squared error (you may choose a different metric)
mse <- mean((valid_data$Yields - predictions)^2)
# Store the result
cv_results[i] <- mse
}
cv_results
# Calculate the mean and standard deviation of the cross-validation results
mean_mse <- mean(cv_results)
std_mse <- sd(cv_results)
# Print the results
cat("Mean MSE:", mean_mse, "\n")
cat("Standard Deviation of MSE:", std_mse, "\n")
# Load necessary libraries
library(boot)
library(ggdist)
library(ggplot2)
library(gghalves)
library(plyr)
help(rt)
# Generate example data, note not normal and non-constant variance
set.seed(812347)
group1 <- rt(30,df=23)*1+10
group2 <- rt(30,df=23)*2.5+12
group3 <- rt(30,df=23)*2+11
n=30
# Generate example data, note not normal and non-constant variance
set.seed(812347)
n=30
group1 <- rt(n,df=23)*1+10
group2 <- rt(n,df=23)*2.5+12
group3 <- rt(n,df=23)*2+11
# Combine the groups
data <- c(group1, group2, group3)
# Create a grouping variable
group <- rep(c("Group1", "Group2", "Group3"), each = n)
# Make dataframe and cleanup other variables
mydata=data.frame(value=data,group=factor(group))
remove(group1,group2,group3,data,group)
View(mydata)
# Plot data
ggplot(mydata, aes(x = group, y = value, fill=group)) +
ggdist::stat_halfeye(
adjust = .5, #custom bandwidth
width = .6,
.width = 0,
justification = -.3,
point_colour = NA) +
geom_boxplot(
width = .25,
outlier.shape = NA
) +
geom_point(
size = 1.3,
alpha = .3,
position = position_jitter(
seed = 1, width = .1
)
) +
coord_cartesian(xlim = c(1.2, NA), clip = "off")+
coord_flip()
# get group means and variances
ddply(mydata,~group,summarise,mean=mean(value),var=var(value))
# Define a function to calculate the pairwise mean differences
pairwise_mean_diff <- function(formula, data=NULL,indices){
dat=data[indices,]
if (inherits(formula, "formula") == FALSE) {
stop("The input model must be a formula.\n")
}
m1=aov(formula,data=dat)
resp=all.vars(formula)[1]
pred=all.vars(formula)[2]
treat=as.factor(m1$model[,pred])
r=length(levels(treat)) # number of levels
g=r*(r-1)/2  #number of all pairwise comparisons
labs=combn(levels(treat),2)
means=tapply(dat[,resp],dat[,pred],mean)
diffs=unname(-combn(means,2,diff)) #minus is to do A-B not B-A
rnames=rep(0,g)
for(i in 1:g){
rnames[i]=paste(labs[1,i],"-",labs[2,i])
}
diffs=setNames(diffs,rnames)
return(diffs)
}
# Perform bootstrap
bootstrap_results <- boot(data=mydata,statistic=pairwise_mean_diff, sim="ordinary",R = 1000,formula=value~group)
# use index=1 for 1st parameter (difference), 2 for 2nd etc
generate_results <- function(results,conf.level=0.90,bonf.adj=T) {
#Create labels
m1=aov(formula,data=dat)
resp=all.vars(formula)[1]
pred=all.vars(formula)[2]
treat=as.factor(m1$model[,pred])
r=length(levels(treat)) # number of levels
g=r*(r-1)/2  #number of all pairwise comparisons
if(bonf.adj==T){conf=round(1-(1-conf.level)/g,3)} #bonferroni adjustment
labs=combn(levels(treat),2)
rnames=rep(0,g)
for(i in 1:g){
rnames[i]=paste(labs[1,i],"-",labs[2,i])
}
# Create a loop to generate plots
num_plots=dim(results$t0)
for (i in 1:num_plots) {
# Create a new plot
plot(results,index=i)
title(paste("  |  Plot:",rnames[i]))
}
# Print the results
cat("Bootstrap results for differences:\n")
cat(rnames,sep ="\n")
print(bootstrap_results)
for (i in 1:num_plots) {
cat("\n")
cat(paste("CI for Pairwise Mean Differences:",rnames[i],"\n"))
print(boot.ci(boot.out=bootstrap_results, type="all",index=i,conf=conf)) # BCa preferred
}
cat(paste("\n Experimentwise confidence level at least",conf.level*100,"%","\n"))
}
# Get confidence intervals for pairwise mean differences
generate_results(bootstrap_results) # plot histogram for difference, do for 1:g, scroll through plots
# Get confidence intervals for pairwise mean differences
generate_results(bootstrap_results) # plot histogram for difference, do for 1:g, scroll through plots
# Define a function to calculate the pairwise mean differences
pairwise_mean_diff <- function(formula, data=NULL,indices){
dat=data[indices,]
if (inherits(formula, "formula") == FALSE) {
stop("The input model must be a formula.\n")
}
m1=aov(formula,data=dat)
resp=all.vars(formula)[1]
pred=all.vars(formula)[2]
treat=as.factor(m1$model[,pred])
r=length(levels(treat)) # number of levels
g=r*(r-1)/2  #number of all pairwise comparisons
labs=combn(levels(treat),2)
means=tapply(dat[,resp],dat[,pred],mean)
diffs=unname(-combn(means,2,diff)) #minus is to do A-B not B-A
rnames=rep(0,g)
for(i in 1:g){
rnames[i]=paste(labs[1,i],"-",labs[2,i])
}
diffs=setNames(diffs,rnames)
return(diffs)
}
View(bootstrap_results)
View(bootstrap_results)
View(mydata)
pairwise_mean_diff(value~group,data=mydata)
indices
